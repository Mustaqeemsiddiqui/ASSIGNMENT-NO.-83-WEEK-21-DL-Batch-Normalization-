{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65fdb29a-fa1b-4d08-bcbf-9d1943386d8b",
   "metadata": {},
   "source": [
    "**Q1. Theory and Concepts:**\n",
    "\n",
    "**1. Explain the concept of batch normalization in the context of Artificial Neural Networks.**\n",
    "\n",
    "**2. Describe the benefits of using batch normalization during training.**\n",
    "\n",
    "**3. Discuss the working principle of batch normalization, including the normalization step and the learnable\n",
    "parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a7c32-ad29-4cb5-a913-9cc438d13f7e",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "**1**\n",
    "\n",
    "### Concept of Batch Normalization in Artificial Neural Networks\n",
    "\n",
    "**Batch Normalization** is a technique used to improve the training efficiency and performance of artificial neural networks (ANNs). It was introduced by Sergey Ioffe and Christian Szegedy in 2015 and has since become a standard practice in deep learning. \n",
    "\n",
    "#### Purpose:\n",
    "The primary purpose of batch normalization is to address the problem of **internal covariate shift**, which refers to the changes in the distribution of activations within a network as training progresses. This shifting distribution can slow down training and require careful tuning of learning rates and initialization.\n",
    "\n",
    "#### How It Works:\n",
    "1. **Normalization of Activations**: Batch normalization normalizes the activations of each layer so that they have a consistent mean and variance. This normalization is applied to each mini-batch during training.\n",
    "\n",
    "2. **Steps Involved**:\n",
    "   - **Compute Statistics**: For each mini-batch, compute the mean and variance of the activations.\n",
    "   - **Normalize**: Subtract the batch mean from the activations and divide by the batch standard deviation to standardize the activations.\n",
    "   - **Scale and Shift**: Apply learnable scale (γ) and shift (β) parameters to the normalized activations to allow the network to learn the optimal scale and shift.\n",
    "\n",
    "3. **Training and Inference**:\n",
    "   - **During Training**: Batch normalization uses the mean and variance computed from the current mini-batch.\n",
    "   - **During Inference**: Use running averages of mean and variance accumulated during training to normalize the activations.\n",
    "\n",
    "#### Example Workflow:\n",
    "1. **Forward Pass**:\n",
    "   - For a given layer's activations, compute the mean and variance for the mini-batch.\n",
    "   - Normalize the activations using these statistics.\n",
    "   - Scale and shift the normalized activations with learnable parameters.\n",
    "\n",
    "2. **Backward Pass**:\n",
    "   - Compute gradients with respect to the normalization parameters (γ and β), as well as the normalized activations.\n",
    "   - Update the parameters using backpropagation.\n",
    "\n",
    "#### Key Components:\n",
    "- **Mini-Batch Mean (\\(\\mu_{\\mathcal{B}}\\))**: Average of activations in the mini-batch.\n",
    "- **Mini-Batch Variance (\\(\\sigma_{\\mathcal{B}}^2\\))**: Variance of activations in the mini-batch.\n",
    "- **Normalization**: Adjust activations to have zero mean and unit variance.\n",
    "- **Scale (\\(\\gamma\\))**: Learnable parameter that scales the normalized activation.\n",
    "- **Shift (\\(\\beta\\))**: Learnable parameter that shifts the normalized activation.\n",
    "\n",
    "### Summary:\n",
    "Batch normalization is a technique that normalizes the activations of each layer to improve training stability and speed. By standardizing the activations within each mini-batch, it reduces the problem of internal covariate shift, allowing the network to train faster and with a higher learning rate. This technique includes computing mini-batch statistics, normalizing activations, and applying learnable scale and shift parameters, making it a crucial component in modern deep learning architectures.\n",
    "\n",
    "\n",
    "**2**\n",
    "\n",
    "### Benefits of Using Batch Normalization During Training\n",
    "\n",
    "Batch normalization provides several key benefits that enhance the training process and performance of artificial neural networks (ANNs):\n",
    "\n",
    "1. **Faster Convergence**:\n",
    "   - **Accelerated Training**: By normalizing the inputs of each layer, batch normalization stabilizes the learning process, allowing the network to converge faster. This is because the network does not have to deal with shifting distributions of activations, which can slow down learning.\n",
    "   - **Higher Learning Rates**: With more stable activations, higher learning rates can be used without the risk of divergence, further speeding up training.\n",
    "\n",
    "2. **Improved Gradient Flow**:\n",
    "   - **Mitigates Vanishing and Exploding Gradients**: Batch normalization helps in maintaining a more stable gradient flow throughout the network. By normalizing activations, it reduces the likelihood of vanishing or exploding gradients, especially in deep networks.\n",
    "   - **Consistent Activations**: Normalized activations prevent gradients from becoming too small or too large, which improves the efficiency of backpropagation.\n",
    "\n",
    "3. **Regularization Effect**:\n",
    "   - **Reduces Overfitting**: Batch normalization introduces a slight noise during training due to the mini-batch statistics used for normalization. This noise acts as a form of regularization, which can reduce overfitting and improve the network's ability to generalize to unseen data.\n",
    "   - **Complementary to Dropout**: While it can reduce the need for dropout, batch normalization can be used alongside dropout for enhanced regularization.\n",
    "\n",
    "4. **Reduced Sensitivity to Initialization**:\n",
    "   - **Easier Network Initialization**: Batch normalization reduces the sensitivity of the network to the choice of initial weights. This is because the normalization of activations makes the network more robust to different initialization schemes, leading to more stable and predictable training.\n",
    "\n",
    "5. **Improved Network Architecture Design**:\n",
    "   - **Deeper Networks**: Batch normalization enables the design of deeper networks by mitigating problems associated with very deep architectures, such as internal covariate shift and gradient issues. This allows for the creation of more complex models with improved performance.\n",
    "   - **More Flexible Architectures**: With batch normalization, the choice of activation functions and network depth becomes less critical, making it easier to experiment with different architectures.\n",
    "\n",
    "6. **Improved Training Stability**:\n",
    "   - **Stable Training Process**: By stabilizing the distribution of activations across layers, batch normalization makes the training process more stable, reducing the fluctuations in the loss function and making the optimization more reliable.\n",
    "\n",
    "7. **Faster Training with Larger Batches**:\n",
    "   - **Efficient Batch Size Utilization**: Batch normalization can improve the efficiency of training with larger batch sizes by making the optimization process more stable and less dependent on the batch size.\n",
    "\n",
    "### Summary:\n",
    "Batch normalization offers several advantages that contribute to more efficient and effective training of neural networks. It accelerates convergence, improves gradient flow, reduces overfitting through a regularization effect, minimizes sensitivity to weight initialization, supports the design of deeper networks, and enhances training stability. These benefits make batch normalization a valuable technique in modern deep learning practices, leading to better performance and faster training of neural networks.\n",
    "\n",
    "\n",
    "**3**\n",
    "\n",
    "### Working Principle of Batch Normalization\n",
    "\n",
    "Batch normalization operates in a systematic manner to stabilize and accelerate the training of artificial neural networks (ANNs). Here's a detailed discussion of how batch normalization works, including the normalization step and the learnable parameters involved.\n",
    "\n",
    "#### 1. Normalization Step:\n",
    "\n",
    "The normalization step in batch normalization involves the following processes:\n",
    "\n",
    "1. **Compute Mini-Batch Statistics**:\n",
    "   - **Mean**: For each feature \\( j \\) in the mini-batch, compute the mean \\( \\mu_{\\mathcal{B},j} \\):\n",
    "     \\[\n",
    "     \\mu_{\\mathcal{B},j} = \\frac{1}{m} \\sum_{i=1}^m x_{i,j}\n",
    "     \\]\n",
    "     where \\( x_{i,j} \\) is the \\( j \\)-th feature of the \\( i \\)-th example in the mini-batch, and \\( m \\) is the mini-batch size.\n",
    "\n",
    "   - **Variance**: Compute the variance \\( \\sigma_{\\mathcal{B},j}^2 \\):\n",
    "     \\[\n",
    "     \\sigma_{\\mathcal{B},j}^2 = \\frac{1}{m} \\sum_{i=1}^m (x_{i,j} - \\mu_{\\mathcal{B},j})^2\n",
    "     \\]\n",
    "\n",
    "2. **Normalize the Activations**:\n",
    "   - **Standardization**: For each activation \\( x_{i,j} \\) in the mini-batch, normalize using the computed mean and variance:\n",
    "     \\[\n",
    "     \\hat{x}_{i,j} = \\frac{x_{i,j} - \\mu_{\\mathcal{B},j}}{\\sqrt{\\sigma_{\\mathcal{B},j}^2 + \\epsilon}}\n",
    "     \\]\n",
    "     where \\( \\epsilon \\) is a small constant added to prevent division by zero and ensure numerical stability.\n",
    "\n",
    "#### 2. Learnable Parameters:\n",
    "\n",
    "After normalization, batch normalization introduces two learnable parameters:\n",
    "\n",
    "1. **Scale Parameter (γ)**:\n",
    "   - **Purpose**: The scale parameter allows the network to adjust the range of the normalized activations. This is necessary because the normalization step standardizes activations to a mean of 0 and variance of 1, which might not always be optimal for the network's performance.\n",
    "   - **Formula**: After normalization, the activations are scaled by γ:\n",
    "     \\[\n",
    "     \\tilde{x}_{i,j} = \\gamma \\cdot \\hat{x}_{i,j}\n",
    "     \\]\n",
    "\n",
    "2. **Shift Parameter (β)**:\n",
    "   - **Purpose**: The shift parameter allows the network to adjust the mean of the normalized activations. This provides the flexibility to shift the normalized values to better fit the learning task.\n",
    "   - **Formula**: After scaling, the activations are shifted by β:\n",
    "     \\[\n",
    "     y_{i,j} = \\gamma \\cdot \\hat{x}_{i,j} + \\beta\n",
    "     \\]\n",
    "\n",
    "   The final output \\( y_{i,j} \\) is the result of applying both the scaling and shifting operations to the normalized activations.\n",
    "\n",
    "#### 3. Training and Inference:\n",
    "\n",
    "- **During Training**:\n",
    "  - **Batch Statistics**: During training, the mean and variance are computed for each mini-batch, and the normalization is performed using these statistics.\n",
    "  - **Parameter Updates**: The parameters γ and β are learned during training through backpropagation, which adjusts these parameters to optimize the network's performance.\n",
    "\n",
    "- **During Inference**:\n",
    "  - **Running Statistics**: During inference, the mean and variance used for normalization are the running averages computed over the entire training dataset. This ensures that the network's behavior is consistent and not dependent on a particular mini-batch.\n",
    "  - **Fixed Parameters**: The learned parameters γ and β are used as-is, and the normalization is applied using the running statistics.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Batch normalization works by normalizing the activations of each layer to a mean of 0 and a variance of 1 for each mini-batch. This normalization step involves computing the mini-batch mean and variance, standardizing the activations, and then applying learnable scale (γ) and shift (β) parameters to the normalized activations. During training, the batch statistics are updated and used for normalization, while during inference, running averages of these statistics are used. This approach improves training stability, accelerates convergence, and allows for the use of higher learning rates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcff395-bd81-4b99-a5ec-44f451027acf",
   "metadata": {},
   "source": [
    "**Q2. Impementation:**\n",
    "\n",
    "**1. Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess it.**\n",
    "\n",
    "**2. Implement a simple feedforward neural network using any deep learning framework/library (e.g.,\n",
    "Tensorlow, xyTorch).**\n",
    "\n",
    "**3. Train the neural network on the chosen dataset without using batch normalization.**\n",
    "\n",
    "**4. Implement batch normalization layers in the neural network and train the model again**\n",
    "\n",
    "**5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and\n",
    "without batch normalization**\n",
    "\n",
    "**6. Discuss the impact of batch normalization on the training process and the performance of the neural\n",
    "network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be96db-c507-4b27-ad19-7447e36f3731",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1fcf70-f59e-4540-91fe-f0c419fcd888",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MNIST\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, random_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "mnist_dataset = MNIST(root='data', train=True, transform=transform, download=True)\n",
    "train_size = int(0.8 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d336ea3-dc1d-4bf7-8c7a-d4e3cd4aa06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch==2.4.0\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchvision) (2.8.8)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.8.0\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchvision) (1.11.1)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.0.0\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchvision) (3.1.2)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0->torchvision) (2022.11.0)\n",
      "Collecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0->torchvision) (1.2.1)\n",
      "Installing collected packages: typing-extensions, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81250d75-d60f-4261-a32b-abca75ceb1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-24.1.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3.1\n",
      "    Uninstalling pip-22.3.1:\n",
      "      Successfully uninstalled pip-22.3.1\n",
      "Successfully installed pip-24.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83ff5e-6b09-47e0-893f-40b7098b2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  # Output layer for 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71664240-4d8f-4244-b09c-a32716221a8c",
   "metadata": {},
   "source": [
    "3. Train the Neural Network Without Batch Normalization\n",
    "\n",
    "Use the above code to train the neural network without batch normalization. Track training and validation performance (accuracy, loss) using validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af2f38-5690-4e2b-9a0a-f97452a1b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNBatchNorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)  # Batch normalization layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)   # Batch normalization layer\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))  # Apply batch normalization\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))  # Apply batch normalization\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate and train the model\n",
    "model_bn = SimpleNNBatchNorm()\n",
    "optimizer_bn = optim.Adam(model_bn.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model with batch normalization\n",
    "train_model(model_bn, train_loader, criterion, optimizer_bn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5edd24-b05c-4100-b336-02558dd82d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss += criterion(outputs, labels).item()\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = loss / len(data_loader)\n",
    "    return accuracy, avg_loss\n",
    "\n",
    "# Evaluate models\n",
    "accuracy_no_bn, loss_no_bn = evaluate_model(model, val_loader)\n",
    "accuracy_bn, loss_bn = evaluate_model(model_bn, val_loader)\n",
    "\n",
    "print(f\"Without Batch Normalization: Accuracy: {accuracy_no_bn:.2f}%, Loss: {loss_no_bn:.4f}\")\n",
    "print(f\"With Batch Normalization: Accuracy: {accuracy_bn:.2f}%, Loss: {loss_bn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc87a54d-37c0-491a-b142-6e527cabbea1",
   "metadata": {},
   "source": [
    "6. Discuss the Impact of Batch Normalization\n",
    "\n",
    "Impact on Training Process and Performance:\n",
    "\n",
    "Faster Convergence:\n",
    "\n",
    "Batch Normalization often leads to faster convergence as it reduces the internal covariate shift and allows the use of higher learning rates. This can result in a quicker reduction of the loss function.\n",
    "\n",
    "Improved Training Stability:\n",
    "\n",
    "Training Stability is enhanced with batch normalization as it normalizes activations, helping prevent the issues associated with vanishing and exploding gradients.\n",
    "\n",
    "Higher Accuracy:\n",
    "\n",
    "Model Performance: \n",
    "In many cases, models with batch normalization achieve higher accuracy and better generalization on the validation set compared to models without batch normalization.\n",
    "\n",
    "Regularization Effect:\n",
    "\n",
    "Regularization:\n",
    "Batch normalization can introduce a regularizing effect due to the noise from mini-batch statistics, potentially reducing overfitting.\n",
    "\n",
    "Sensitivity to Initialization:\n",
    "\n",
    "Less Sensitivity:\n",
    "Models with batch normalization are typically less sensitive to the choice of weight initialization, making the training process more robust.\n",
    "\n",
    "By comparing the performance of the neural network with and without batch normalization, you will likely observe that batch normalization provides benefits in terms of convergence speed, training stability, and overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27577d-2e74-4693-948e-062e5ae3c073",
   "metadata": {},
   "source": [
    "**Q3. Experimentation and anaysis**\n",
    "\n",
    "**.Experiment with different batch sizes and observe the effect on the training dynamics and model\n",
    "performance.**\n",
    "\n",
    "**2.discuss the advantages and potential limitations of batch normalization in improving the training of\n",
    "neural networks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a3f22-e1ee-42bd-af1d-ecc19916e834",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "### Experimentation and Analysis\n",
    "\n",
    "#### 1. Experiment with Different Batch Sizes\n",
    "\n",
    "**Goal:** Observe how different batch sizes affect the training dynamics and model performance.\n",
    "\n",
    "\n",
    "#### 2. Discussion on Batch Normalization\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- **Improved Training Speed**: Batch normalization can accelerate training by reducing internal covariate shift, allowing the network to converge faster.\n",
    "- **Stable Learning**: It helps stabilize the learning process, making the network less sensitive to initialization and learning rate choices.\n",
    "- **Regularization Effect**: Acts as a form of regularization, often reducing the need for other regularization techniques such as dropout. This can lead to improved generalization.\n",
    "- **Higher Learning Rates**: Allows the use of higher learning rates, which can further speed up the training process.\n",
    "\n",
    "**Potential Limitations:**\n",
    "\n",
    "- **Batch Size Dependency**: Performance can be sensitive to batch size. Very small batch sizes might not provide accurate statistics for normalization.\n",
    "- **Additional Computational Overhead**: Batch normalization introduces extra computations during both training and inference, which can slow down the model if not managed properly.\n",
    "- **Complexity in Implementation**: Adding batch normalization layers increases the complexity of the model implementation.\n",
    "- **Unstable at Inference Time**: There can be issues during inference if the batch statistics differ significantly from the training statistics. Proper handling of running statistics is required.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Batch normalization is a powerful technique to improve the training dynamics and performance of neural networks. Through experimentation with different batch sizes, one can observe how batch normalization influences training speed, stability, and generalization capabilities. However, it also introduces additional complexity and computational overhead, which need to be balanced based on the specific use case and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a1f29-0b52-466d-add9-d61a9a2ef011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2022.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Collecting triton==3.0.0 (from torch)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a5d20-046b-4e48-bd02-16da2f054d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "mnist_dataset = MNIST(root='data', train=True, transform=transform, download=True)\n",
    "train_size = int(0.8 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Define the model with batch normalization\n",
    "class SimpleNNBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNBatchNorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        \n",
    "        val_loss = evaluate_model(model, val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "    avg_loss = loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Experiment with different batch sizes\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "results = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model_bn = SimpleNNBatchNorm()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model_bn.parameters(), lr=0.001)\n",
    "    \n",
    "    print(f\"\\nTraining with batch size: {batch_size}\")\n",
    "    train_losses, val_losses = train_model(model_bn, train_loader, criterion, optimizer)\n",
    "    \n",
    "    results[batch_size] = {'train_loss': train_losses, 'val_loss': val_losses}\n",
    "\n",
    "# Plotting the results\n",
    "for batch_size in batch_sizes:\n",
    "    plt.plot(results[batch_size]['train_loss'], label=f'Train Loss (Batch {batch_size})')\n",
    "    plt.plot(results[batch_size]['val_loss'], label=f'Val Loss (Batch {batch_size})')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss with Different Batch Sizes')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b01443-ef7d-44dc-886d-53b68b983310",
   "metadata": {},
   "source": [
    "**Submission Guideines**\n",
    "\n",
    "1. Complete the assignment in a Jupyter Notebook\n",
    "\n",
    "2 Include necessary comments and explanations to make your code understandable\n",
    "\n",
    "3.provide visualizations, tables, and explanations for your analysis and findings\n",
    "\n",
    "4. Create a GitHub repository to host your assignment files\n",
    "\n",
    "5. Rename your Jupyter Notebook file usin0 the format \"date_month_topic.ipynb\" (e.0.,\n",
    "\"12_July_Re0ression.ipynb\")\n",
    "\n",
    "6.place your Jupyter Notebook file (.ipynb) in the repository\n",
    "\n",
    "7.Ensure that the notebook runs without errors\n",
    "\n",
    "8. Commit and push any additional files or resources required to run your code (if applicable) to the\n",
    "repository\n",
    "\n",
    "9. Make sure the repository is publicly accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf264ffe-2232-4fc2-a385-23586c8c2390",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "### Submission Guidelines and Assignment Completion Steps\n",
    "\n",
    "1. **Complete the Assignment in a Jupyter Notebook**:\n",
    "    - Make sure your Jupyter Notebook includes the complete implementation, experimentation, and analysis.\n",
    "\n",
    "2. **Include Comments and Explanations**:\n",
    "    - Add necessary comments and markdown cells to explain your code, methodology, and findings clearly.\n",
    "\n",
    "3. **Provide Visualizations, Tables, and Explanations**:\n",
    "    - Include visualizations (e.g., plots of training/validation loss), tables (e.g., comparison of different batch sizes), and thorough explanations for your analysis and findings.\n",
    "\n",
    "4. **Create a GitHub Repository**:\n",
    "    - If you don't have a GitHub account, create one at [GitHub](https://github.com/).\n",
    "    - Create a new repository for your assignment.\n",
    "\n",
    "5. **Rename Your Jupyter Notebook**:\n",
    "    - Use the format `date_month_topic.ipynb` (e.g., `12_July_BatchNormalization.ipynb`).\n",
    "\n",
    "6. **Place Your Notebook in the Repository**:\n",
    "    - Add your Jupyter Notebook file to the repository.\n",
    "\n",
    "7. **Ensure the Notebook Runs Without Errors**:\n",
    "    - Before submission, run all cells in your notebook to ensure there are no errors and all outputs are as expected.\n",
    "\n",
    "8. **Commit and Push Additional Files**:\n",
    "    - If your code requires any additional files or resources, make sure to commit and push them to the repository.\n",
    "\n",
    "9. **Make the Repository Publicly Accessible**:\n",
    "    - Ensure the repository settings allow public access.\n",
    "\n",
    "### Example Directory Structure\n",
    "Your repository should look something like this:\n",
    "\n",
    "```\n",
    "my-assignment-repo/\n",
    "│\n",
    "├── 12_July_BatchNormalization.ipynb\n",
    "├── README.md (optional, but recommended for brief description)\n",
    "├── data/ (if you have any data files)\n",
    "│   └── dataset.csv\n",
    "├── plots/ (if you save any plots as images)\n",
    "│   └── training_loss_plot.png\n",
    "└── requirements.txt (if there are specific libraries/dependencies)\n",
    "```\n",
    "\n",
    "### Example README.md\n",
    "A `README.md` file can provide a brief overview of your assignment and instructions on how to run your notebook. Here’s an example:\n",
    "\n",
    "```markdown\n",
    "# Batch Normalization in Neural Networks\n",
    "\n",
    "This repository contains the implementation and analysis of batch normalization in neural networks. The main objective of this assignment is to understand the impact of batch normalization on training performance.\n",
    "\n",
    "## Files\n",
    "- `12_July_BatchNormalization.ipynb`: Jupyter Notebook with the complete implementation, experimentation, and analysis.\n",
    "- `data/`: Directory containing dataset files (if applicable).\n",
    "- `plots/`: Directory containing plot images (if applicable).\n",
    "- `requirements.txt`: List of dependencies required to run the notebook.\n",
    "\n",
    "## Usage\n",
    "1. Clone the repository:\n",
    "   ```bash\n",
    "   git clone https://github.com/yourusername/my-assignment-repo.git\n",
    "   ```\n",
    "2. Navigate to the repository directory:\n",
    "   ```bash\n",
    "   cd my-assignment-repo\n",
    "   ```\n",
    "3. Install dependencies:\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "4. Open and run the Jupyter Notebook:\n",
    "   ```bash\n",
    "   jupyter notebook 12_July_BatchNormalization.ipynb\n",
    "   ```\n",
    "\n",
    "## Results\n",
    "- The notebook contains detailed visualizations, tables, and explanations of the impact of batch normalization on training dynamics and model performance.\n",
    "```\n",
    "\n",
    "### Final Steps\n",
    "- **Commit and Push**: Ensure all changes are committed and pushed to your GitHub repository.\n",
    "- **Check Accessibility**: Verify that the repository is publicly accessible by opening it in an incognito window or logging out of GitHub and checking the repository link.\n",
    "\n",
    "### Example Code and Explanation\n",
    "\n",
    "Here’s a refined version of the previous code, including comments and visualizations:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "mnist_dataset = MNIST(root='data', train=True, transform=transform, download=True)\n",
    "train_size = int(0.8 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Define the model with batch normalization\n",
    "class SimpleNNBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNBatchNorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        \n",
    "        val_loss = evaluate_model(model, val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "    avg_loss = loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Experiment with different batch sizes\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "results = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model_bn = SimpleNNBatchNorm()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model_bn.parameters(), lr=0.001)\n",
    "    \n",
    "    print(f\"\\nTraining with batch size: {batch_size}\")\n",
    "    train_losses, val_losses = train_model(model_bn, train_loader, criterion, optimizer)\n",
    "    \n",
    "    results[batch_size] = {'train_loss': train_losses, 'val_loss': val_losses}\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "for batch_size in batch_sizes:\n",
    "    plt.plot(results[batch_size]['train_loss'], label=f'Train Loss (Batch {batch_size})')\n",
    "    plt.plot(results[batch_size]['val_loss'], label=f'Val Loss (Batch {batch_size})')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss with Different Batch Sizes')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This code will give you the training and validation loss plots for different batch sizes. Make sure to provide a discussion on the observed effects and the advantages and limitations of batch normalization in your notebook.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Complete the Experimentation**: Ensure you run experiments for all mentioned batch sizes and document the results.\n",
    "2. **Analyze Results**: Add a detailed analysis of the results in markdown cells within your Jupyter Notebook.\n",
    "3. **Push to GitHub**: Follow the submission guidelines to push your notebook and any required files to GitHub.\n",
    "4. **Submit**: Share the GitHub repository link as required for your assignment submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d368b9-af0f-4d76-a75a-0e1590691416",
   "metadata": {},
   "source": [
    "**Grading Criteria**\n",
    "\n",
    "1.Understanding of Batch Normalization (30%)\n",
    "\n",
    "2.Implementation and Experimental _nalysis (40%)\n",
    "\n",
    "3._nalysis and Interpretation (20%)\n",
    "\n",
    "4.Organization, Clarity, and Presentation (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb7a20d-5b8c-4428-9aa8-e733ec136821",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "### Grading Criteria and Assignment Guide\n",
    "\n",
    "To excel in your assignment and meet the grading criteria, here’s a structured guide to follow:\n",
    "\n",
    "#### 1. Understanding of Batch Normalization (30%)\n",
    "- **Explanation of Concept**: Clearly define batch normalization and its role in neural networks.\n",
    "- **Benefits of Batch Normalization**: Describe the advantages during training, including how it stabilizes the learning process and allows for higher learning rates.\n",
    "- **Working Principle**: Detail the steps involved in batch normalization, including normalization and learnable parameters (γ and β).\n",
    "\n",
    "#### 2. Implementation and Experimental Analysis (40%)\n",
    "- **Dataset Selection and Preprocessing**: Choose a suitable dataset (e.g., MNIST, CIFAR-10), and preprocess it effectively.\n",
    "- **Model Implementation**:\n",
    "  - Implement a simple feedforward neural network without batch normalization.\n",
    "  - Implement a neural network with batch normalization layers.\n",
    "- **Training and Evaluation**:\n",
    "  - Train both models and compare their performance (accuracy, loss).\n",
    "  - Experiment with different batch sizes and document the effects on training dynamics and performance.\n",
    "- **Code Quality**: Ensure your code is well-commented, modular, and adheres to best practices.\n",
    "\n",
    "#### 3. Analysis and Interpretation (20%)\n",
    "- **Performance Comparison**: Compare training and validation performance metrics (e.g., accuracy, loss) between models with and without batch normalization.\n",
    "- **Effect of Batch Sizes**: Analyze the impact of different batch sizes on the training process and model performance.\n",
    "- **Discussion**: Provide a detailed discussion on the results, including the advantages and limitations of batch normalization in improving the training of neural networks.\n",
    "\n",
    "#### 4. Organization, Clarity, and Presentation (10%)\n",
    "- **Structure and Flow**: Organize your notebook logically with clear sections for each part of the assignment.\n",
    "- **Clarity**: Use markdown cells to explain your methodology, code, and findings.\n",
    "- **Visualizations**: Include plots and tables to support your analysis and make the findings easily understandable.\n",
    "- **Readability**: Ensure the notebook is easy to read and navigate, with consistent formatting and style.\n",
    "\n",
    "### Example Notebook Structure\n",
    "\n",
    "1. **Introduction**\n",
    "   - Brief overview of the assignment and objectives.\n",
    "\n",
    "2. **Understanding Batch Normalization**\n",
    "   - Explanation of batch normalization.\n",
    "   - Benefits of using batch normalization.\n",
    "   - Working principle and learnable parameters.\n",
    "\n",
    "3. **Dataset and Preprocessing**\n",
    "   - Description of the chosen dataset.\n",
    "   - Preprocessing steps.\n",
    "\n",
    "4. **Model Implementation**\n",
    "   - Implementation of the feedforward neural network without batch normalization.\n",
    "   - Implementation of the feedforward neural network with batch normalization.\n",
    "\n",
    "5. **Training and Evaluation**\n",
    "   - Training the models.\n",
    "   - Comparing performance metrics.\n",
    "   - Experimenting with different batch sizes.\n",
    "\n",
    "6. **Analysis and Interpretation**\n",
    "   - Detailed analysis of the results.\n",
    "   - Discussion on the impact of batch normalization.\n",
    "   - Comparison of training dynamics and performance with different batch sizes.\n",
    "\n",
    "7. **Conclusion**\n",
    "   - Summarize key findings and insights.\n",
    "   - Reflect on the advantages and limitations of batch normalization.\n",
    "\n",
    "8. **References**\n",
    "   - List any references or resources used.\n",
    "\n",
    "### Implementation and Experimentation Example\n",
    "\n",
    "Here is an updated version of the code for training and evaluating the models, including batch normalization, experimentation with different batch sizes, and visualization:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "mnist_dataset = MNIST(root='data', train=True, transform=transform, download=True)\n",
    "train_size = int(0.8 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Define the model without batch normalization\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the model with batch normalization\n",
    "class SimpleNNBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNBatchNorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.bn1(self.fc1(x)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        \n",
    "        val_loss = evaluate_model(model, val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "    avg_loss = loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# Experiment with different batch sizes\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "results = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Model without batch normalization\n",
    "    model = SimpleNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    print(f\"\\nTraining without batch normalization, batch size: {batch_size}\")\n",
    "    train_losses, val_losses = train_model(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    results[f'no_bn_{batch_size}'] = {'train_loss': train_losses, 'val_loss': val_losses}\n",
    "    \n",
    "    # Model with batch normalization\n",
    "    model_bn = SimpleNNBatchNorm()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model_bn.parameters(), lr=0.001)\n",
    "    \n",
    "    print(f\"\\nTraining with batch normalization, batch size: {batch_size}\")\n",
    "    train_losses, val_losses = train_model(model_bn, train_loader, criterion, optimizer)\n",
    "    \n",
    "    results[f'bn_{batch_size}'] = {'train_loss': train_losses, 'val_loss': val_losses}\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "for key, value in results.items():\n",
    "    plt.plot(value['train_loss'], label=f'Train Loss ({key})')\n",
    "    plt.plot(value['val_loss'], label=f'Val Loss ({key})')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss with Different Batch Sizes and Batch Normalization')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Discussion Points\n",
    "\n",
    "- **Advantages of Batch Normalization**:\n",
    "  - Reduces internal covariate shift.\n",
    "  - Allows for higher learning rates.\n",
    "  - Regularizes the model, reducing the need for dropout.\n",
    "  - Stabilizes the learning process.\n",
    "\n",
    "- **Potential Limitations**:\n",
    "  - Additional computational overhead during training.\n",
    "  - May not be as effective for very small batch sizes.\n",
    "  - Requires tuning of additional hyperparameters (γ and β).\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- Summarize your key findings, highlighting the impact of batch normalization on the training dynamics and model performance.\n",
    "- Reflect on the benefits and limitations observed during your experiments.\n",
    "\n",
    "By following this structured approach and adhering to the grading criteria, you will be well-prepared to complete and submit your assignment effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43913a-93cd-43da-832a-cd95dfaddb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9053690-afab-4e89-9c7e-0d9b0b529c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eb8907-6b3a-4669-a61f-5055fe9c4f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
